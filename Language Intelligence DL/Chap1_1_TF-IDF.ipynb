{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF(Term Frequency-Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "  \n",
    "TF-IDF는 단어의 빈도와 역 문서 빈도(문서의 빈도에 특정 식을 취함)를 사용해 DTM내 각 단어들마다 중요 정도를 가중치로 주는 방법으로 식은 다음과 같다.  \n",
    "$$TF-IDF = TF(t,d) * IDF(t, D)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. tf(d, t) : 특정 문서 d에서의 특정 단어 t의 등장 횟수\n",
    "  \n",
    "#### 2. df(t) : 특정 단어 t가 등장한 문서의 수\n",
    "특정 단어가 각 문서, 또는 문서들에서 몇 번 등장했는지는 관심가지지 않음.  \n",
    "오직 특정 단어 t가 등장한 문서의 수에만 관심을 가짐.\n",
    "\n",
    "#### 3. idf(t) : df(t)에 반비례하는 수\n",
    "$$idf(t) = log(\\frac{n}{1+df(t)})$$  \n",
    "**log를 사용하는 이유)**  \n",
    "log를 사용하는 이유는 만약 IDF를 단순히 DF의 역수로 사용한다면 총 문서의 수 n이 커질수록, IDF의 값은 기하급수적으로 커지는데 이를 방지하기 위해 사용.  \n",
    "  \n",
    "예를 들어, n = 1,000,000일 때  \n",
    "$idf(t) = log(\\frac{n}{df(t)})$\n",
    "$n = 1,000,000$\n",
    "|단어 $t$|$df(t)$|$idf(d, t)$|\n",
    "|---|---|---|\n",
    "|word1|1|6|\n",
    "|word2|100|4|\n",
    "|word3|1,000|3|\n",
    "|word4|10,000|2|\n",
    "|word5|100,000|1|\n",
    "|word6|1,000,000|0|\n",
    "  \n",
    "log를 사용하지 않으면 다음과 같다.\n",
    "|단어 $t$|$df(t)$|$idf(t)$|\n",
    "|---|---|---|\n",
    "|word1|1|1,000,000|\n",
    "|word2|100|10,000|\n",
    "|word3|1,000|1,000|\n",
    "|word4|10,000|100|\n",
    "|word5|100,000|10|\n",
    "|word6|1,000,000|1|\n",
    "  \n",
    "**1을 더하는 이유**\n",
    "특정 단어가 전체 문서에서 등장하지 않을 경우에 분모가 0이 되는 상황을 방지\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어 표현(Word Representation, Word Embedding, Word Vector)\n",
    "  \n",
    "**Q) 어떻게 텍스트를 표현해야 자연어 처리 모델에 적용할 수 있을까?**  \n",
    "언어적인 특성을 반영해 단어를 수치화 하는 방법 -> \n",
    "**<span style=\"color:orange; \">벡터</span>**\n",
    "  \n",
    "**데이터 표현**\n",
    "1. 기본: One-Hot Encoding -> 하지만 이 방식은 자연어 단어 표현에는 부적합\n",
    "    - 단어의 의미나 특성을 표현할 수 없음\n",
    "    - 단어의 수가 매우 많으므로 \n",
    "    **<span style=\"color:orange\">고차원 저밀도 벡터</span>**\n",
    "    를 구성함  \n",
    "2. 벡터의 크기가 작으면서 단어의 의미를 표현하는 방법 -> 분포가설에 기반\n",
    "    - 분포가설(Distributed Hypothesis): 같은 문맥의 단어, 즉 비슷한 위치에 나오는 단어는 비슷한 의미를 가진다.\n",
    "    - 표현법\n",
    "        - **카운트 기반 방법(Count-based):**\n",
    "        특정 문맥 안에서 단어들이 동시에 등장하는 횟수를 직접 셈\n",
    "        - **예측 방법(Predictive):**\n",
    "        신경망 등을 통해 문맥 안의 단어들을 예측\n",
    "  \n",
    "**단어 표현의 정의**\n",
    "텍스트가 얼마나 유사한지를 표현하는 방식\n",
    "  \n",
    "**유사도 판단의 다양한 방식**\n",
    "- 단순히 같은 단어의 개수를 사용해 유사도를 판단\n",
    "- 형태소로 나누어 형태소를 비교\n",
    "\n",
    "**딥러닝 기반의 유사도 판단**\n",
    "- 텍스트를 벡터화 한 후 벡터화된 각 문장 간의 유사도를 측정\n",
    "- 대표적 방식 : Jaccard Similarity, Cosine Similarity, Euclidean Similarity, Manhatten Similarity(Jaccard Similarity는 벡터화 없이 바로 측정 가능)\n",
    "  \n",
    "  \n",
    "### 1. Jaccard Similarity\n",
    "- 두 문장을 각각 단어의 집합으로 만든 뒤 두 집합을 통해 유사도 측정(0과 1사이의 값)\n",
    "- 측정 방법 : $A/B$\n",
    "    - $A$: 두 집합의 교집합인 공통된 단어의 개수\n",
    "    - $B$: 집합이 가지는 단어의 개수  \n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Jaccard Similarity\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "print('jaccard similarity:', jaccard_similarity_score(np.array([1, 3, 2]), np.array([1, 4, 5])))\n",
    "```\n",
    "\n",
    "### 2. Cosine Similarity\n",
    "- 두 개의 벡터값에서 <span style=\"color: orange\">코사인 각도</span>를 구하는 방법\n",
    "- -1에서 1사이의 값을 가짐\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "sent = (\"휴일 인 오늘 도 서쪽 을 중심 으로 폭염 이 이어졌는데요, 내일 은 반가운 비 소식 이 있습니다.\", \"폭염 을 피해서 휴일 에 놀러왔다가 갑작스런 비 로 인해 망연자실 하고 있습니 다.\")\n",
    "tfidf_vectorizer = TfidfVetorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(sent)     # document vectorization\n",
    "\n",
    "print(tfidf_matrix)\n",
    "print('First sentence')\n",
    "print(tfidf_matrix[0])\n",
    "print('Second sentence')\n",
    "print(tfidf_matrix[1])\n",
    "\n",
    "# Cosien Similarity\n",
    "from sklearn.metrics import cosine_similarity\n",
    "print('Cosine similarity:', cosine_similarity(tfidf_matrix[0], tfidf_matrix[1]))\n",
    "```\n",
    "\n",
    "### 3. Euclidean Similarity\n",
    "- 두 벡터 간의 거리로 유사도를 판단(기준: Euclidean distance)\n",
    "  \n",
    "```python\n",
    "import numpy as np\n",
    "from skelarn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "sent = (\"휴일 인 오늘 도 서쪽 을 중심 으로 폭염 이 이어졌는데요, 내일 은 반가운 비 소식 이 있습니다.\", \"폭염 을 피해서 휴일 에 놀러왔다가 갑작스런 비 로 인해 망연자실 하고 있습니 다.\")\n",
    "tfidf_vectorizer = TfidfVetorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(sent)     # document vectorization\n",
    "print(tfidf_matrix)\n",
    "\n",
    "idf = tfidf_vectorizer.idf_\n",
    "print(dict(zip(tfidf_vectorizer.get_feature_names(), idf)))\n",
    "\n",
    "# Euclidean distance\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "print('Euclidean similarity:', euclidean_distances(tfidf_matrix[0], tfidf_matrix[1]))\n",
    "\n",
    "def l1_normalize(v):\n",
    "    norm = np.sum(v)\n",
    "    return v/norm\n",
    "\n",
    "tfidf_norm_l1 = l1_normalize(tfidf_matrix)\n",
    "print('Euclidean similarity (norm):', euclidean_distances(tfidf_norm_l1[0], tfidf_norm_l1[1]))\n",
    "```\n",
    "\n",
    "### 4. Manhattan Similarity\n",
    "- 두 벡터 간의 거리로 유사도를 판단(기준: Manhattan distance)\n",
    "\n",
    "**Euclidean vs Manhattan**\n",
    "  \n",
    "<div style=\"text-align:center\">\n",
    "    <img src = \"https://github.com/Ha-coding-user/aivle_study/blob/main/Language%20Intelligence%20DL/image/Euclidean_Manttan.jpg?raw=true\">\n",
    "</div>\n",
    "  \n",
    "```python\n",
    "import numpy as np\n",
    "from skelarn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "sent = (\"휴일 인 오늘 도 서쪽 을 중심 으로 폭염 이 이어졌는데요, 내일 은 반가운 비 소식 이 있습니다.\", \"폭염 을 피해서 휴일 에 놀러왔다가 갑작스런 비 로 인해 망연자실 하고 있습니 다.\")\n",
    "tfidf_vectorizer = TfidfVetorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(sent)     # document vectorization\n",
    "print(tfidf_matrix)\n",
    "\n",
    "idf = tfidf_vectorizer.idf_\n",
    "print(dict(zip(tfidf_vectorizer.get_feature_names(), idf)))\n",
    "\n",
    "# Euclidean distance\n",
    "from sklearn.metrics.pairwise import manhattan_distances\n",
    "print('Euclidean similarity:', manhattan_distances(tfidf_matrix[0], tfidf_matrix[1]))\n",
    "\n",
    "def l1_normalize(v):\n",
    "    norm = np.sum(v)\n",
    "    return v/norm\n",
    "\n",
    "tfidf_norm_l1 = l1_normalize(tfidf_matrix)\n",
    "print('Manhattan similarity (norm):', manhattan_distances(tfidf_norm_l1[0], tfidf_norm_l1[1]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RSS에서 TF 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: feedparser in c:\\users\\gkwjd\\anaconda3\\lib\\site-packages (6.0.10)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\gkwjd\\anaconda3\\lib\\site-packages (from feedparser) (1.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\gkwjd\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\gkwjd\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\gkwjd\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\gkwjd\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\gkwjd\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\gkwjd\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: newspaper3k in c:\\users\\gkwjd\\anaconda3\\lib\\site-packages (0.2.8)\n",
      "Requirement already satisfied: tldextract>=2.0.1 in c:\\users\\gkwjd\\anaconda3\\lib\\site-packages (from newspaper3k) (3.2.0)\n",
      "Requirement already satisfied: PyYAML>=3.11 in c:\\users\\gkwjd\\anaconda3\\lib\\site-packages (from newspaper3k) (6.0)\n",
      "Requirement already satisfied: nltk>=3.2.1 in c:\\users\\gkwjd\\anaconda3\\lib\\site-packages (from newspaper3k) (3.7)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\gkwjd\\anaconda3\\lib\\site-packages (from newspaper3k) (2.8.2)\n",
      "Requirement already satisfied: requests>=2.10.0 in c:\\users\\gkwjd\\anaconda3\\lib\\site-packages (from newspaper3k) (2.28.1)\n",
      "Requirement already satisfied: feedparser>=5.2.1 in c:\\users\\gkwjd\\anaconda3\\lib\\site-packages (from newspaper3k) (6.0.10)\n",
      "Requirement already satisfied: cssselect>=0.9.2 in c:\\users\\gkwjd\\anaconda3\\lib\\site-packages (from newspaper3k) (1.1.0)\n",
      "Requirement already satisfied: jieba3k>=0.35.1 in c:\\users\\gkwjd\\anaconda3\\lib\\site-packages (from newspaper3k) (0.35.1)\n",
      "Requirement already satisfied: tinysegmenter==0.3 in c:\\users\\gkwjd\\anaconda3\\lib\\site-packages (from newspaper3k) (0.3)\n",
      "Requirement already satisfied: feedfinder2>=0.0.4 in c:\\users\\gkwjd\\anaconda3\\lib\\site-packages (from newspaper3k) (0.0.4)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in c:\\users\\gkwjd\\appdata\\roaming\\python\\python39\\site-packages (from newspaper3k) (10.0.0)\n",
      "Requirement already satisfied: lxml>=3.6.0 in c:\\users\\gkwjd\\anaconda3\\lib\\site-packages (from newspaper3k) (4.9.1)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in c:\\users\\gkwjd\\anaconda3\\lib\\site-packages (from newspaper3k) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\gkwjd\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.3.1)\n",
      "Requirement already satisfied: six in c:\\users\\gkwjd\\anaconda3\\lib\\site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\gkwjd\\anaconda3\\lib\\site-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\gkwjd\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\gkwjd\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (4.64.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\gkwjd\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (1.3.2)\n",
      "Requirement already satisfied: click in c:\\users\\gkwjd\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (8.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\gkwjd\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\gkwjd\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gkwjd\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gkwjd\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2022.9.14)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\gkwjd\\anaconda3\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (1.5.1)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\gkwjd\\anaconda3\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (3.6.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\gkwjd\\anaconda3\\lib\\site-packages (from click->nltk>=3.2.1->newspaper3k) (0.4.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\gkwjd\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\gkwjd\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\gkwjd\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\gkwjd\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\gkwjd\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\gkwjd\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in c:\\users\\gkwjd\\anaconda3\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\users\\gkwjd\\anaconda3\\lib\\site-packages (from konlpy) (1.22.4)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\users\\gkwjd\\anaconda3\\lib\\site-packages (from konlpy) (4.9.1)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in c:\\users\\gkwjd\\anaconda3\\lib\\site-packages (from konlpy) (1.4.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\gkwjd\\anaconda3\\lib\\site-packages (from JPype1>=0.7.0->konlpy) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\gkwjd\\appdata\\roaming\\python\\python39\\site-packages (from packaging->JPype1>=0.7.0->konlpy) (3.1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\gkwjd\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\gkwjd\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\gkwjd\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\gkwjd\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\gkwjd\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\gkwjd\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install feedparser\n",
    "!pip install newspaper3k\n",
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser               # RSS에서 xml 태그별 정보추출(예: title, link...)\n",
    "from newspaper import Article   # 인터넷 신문기사 분석(아래: \"Article()\"를 사용하기 위함)\n",
    "from konlpy.tag import Okt      # 한국어 형태소 분석기(주어진 문장에서 명사만 추출)\n",
    "from collections import Counter # 명사 추출 후에 본문에 몇 번 나오는지 확인(TF 계산용)\n",
    "from operator import eq         # 키워드를 입력받을 때 그 키워드가 본문에 있는지 확인하기 위해\n",
    "from bs4 import BeautifulSoup   # 글에 존재할지 모르는 html 태그 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawl RSS http://rss.etnews.com/Section901.xml\n",
      "Crawl RSS http://rss.etnews.com/Section902.xml\n",
      "Duplicated Article: [포토] 동료들과 단체사진 촬영하는 이예원\n",
      "Crawl RSS http://rss.etnews.com/Section903.xml\n",
      "Duplicated Article: 달나라에서 숙박?…NASA “2040년까지 달에 민간주택 건설한다”\n",
      "Crawl RSS http://rss.etnews.com/Section904.xml\n",
      "Duplicated Article: 2000년 전 로마 유적지에서 발견된 '핑크빛 아이섀도'\n",
      "Duplicated Article: 제임스웹, 오리온 성운서 목성 크기의 신비한 천체 발견\n",
      "Duplicated Article: 현대차, 伊이베코그룹과 수소버스 첫 공개…한번 충전에 450㎞ 주행\n",
      "Duplicated Article: 할머니 뇌 속에 80년간 '바늘' 꽂혀 있었다\n",
      "Duplicated Article: 바이든 대통령 반려견, '사람 공격 11번'에 결국 백악관서 퇴출\n",
      "Duplicated Article: '소록도 천사' 故 마가렛 간호사, 자국 의대에 시신 기증…마지막까지 베풀었다\n",
      "Duplicated Article: 달나라에서 숙박?…NASA “2040년까지 달에 민간주택 건설한다”\n",
      "[{'title': \"2000년 전 로마 유적지에서 발견된 '핑크빛 아이섀도'\", 'link': 'https://www.etnews.com/20231006000352'}, {'title': '제임스웹, 오리온 성운서 목성 크기의 신비한 천체 발견', 'link': 'https://www.etnews.com/20231006000421'}, {'title': '현대차, 伊이베코그룹과 수소버스 첫 공개…한번 충전에 450㎞ 주행', 'link': 'https://www.etnews.com/20231008000006'}, {'title': \"할머니 뇌 속에 80년간 '바늘' 꽂혀 있었다\", 'link': 'https://www.etnews.com/20231006000311'}, {'title': \"바이든 대통령 반려견, '사람 공격 11번'에 결국 백악관서 퇴출\", 'link': 'https://www.etnews.com/20231006000154'}, {'title': \"'소록도 천사' 故 마가렛 간호사, 자국 의대에 시신 기증…마지막까지 베풀었다\", 'link': 'https://www.etnews.com/20231006000205'}, {'title': '달나라에서 숙박?…NASA “2040년까지 달에 민간주택 건설한다”', 'link': 'https://www.etnews.com/20231006000081'}, {'title': \"아이브, '글로벌 향한 MZ워너비 표 오색 자기애 축포'(콘서트 종합)\", 'link': 'https://www.etnews.com/20231008000209'}, {'title': '신상진 성남시장, 1만여 명의 시민과 함께 가을밤 정취 느껴', 'link': 'https://www.etnews.com/20231008000208'}, {'title': '천재교과서 밀크티X기출로 중간고사 족집게 특강 오픈', 'link': 'https://www.etnews.com/20231006000217'}, {'title': '[포토] 이글상 수상하는 김민별', 'link': 'https://www.etnews.com/20231008000207'}, {'title': '[포토] 가족들과 기념촬영하는 이예원', 'link': 'https://www.etnews.com/20231008000206'}, {'title': '[포토] 챔피언조를 따르는 갤러리', 'link': 'https://www.etnews.com/20231008000205'}, {'title': '[포토] 우승확정후 이뻐하는 이예원', 'link': 'https://www.etnews.com/20231008000204'}, {'title': '[포토] 캐디와 우승 기쁨을 나누는 이예원', 'link': 'https://www.etnews.com/20231008000203'}, {'title': '[포토] 하이트진로 챔피언십 우승 축하받는 이예원', 'link': 'https://www.etnews.com/20231008000202'}, {'title': '[포토] 하이트진로 챔피언십 맥주 세리머니하는 이예원', 'link': 'https://www.etnews.com/20231008000201'}, {'title': '[포토] 하이트진로 챔피언십 7언더파 우승 자축하는 이예원', 'link': 'https://www.etnews.com/20231008000200'}, {'title': '[포토] 하이트진로 챔피언십 7언더파 우승하는 이예원', 'link': 'https://www.etnews.com/20231008000199'}, {'title': '[포토] 시즌 3승을 알리는 이예원', 'link': 'https://www.etnews.com/20231008000198'}, {'title': '[포토] 매니저먼트 직원들과 기념촬영하는 이예원', 'link': 'https://www.etnews.com/20231008000197'}, {'title': '[포토] 남민지 대표와 기념촬영하는 이예원', 'link': 'https://www.etnews.com/20231008000196'}, {'title': '[포토] 우승후 기념촬영하는 이예원', 'link': 'https://www.etnews.com/20231008000195'}, {'title': '[포토] 맥주마시는 세리머니하는 이예원', 'link': 'https://www.etnews.com/20231008000194'}, {'title': '[포토] 메이저대회 우승 핸드프린팅하는 이예원', 'link': 'https://www.etnews.com/20231008000193'}, {'title': '[포토] 메이저대회 첫우승하는 이예원', 'link': 'https://www.etnews.com/20231008000192'}, {'title': '[포토] 동료들과 단체사진 촬영하는 이예원', 'link': 'https://www.etnews.com/20231008000191'}, {'title': '[포토] 우승 소감 말하는 이예원', 'link': 'https://www.etnews.com/20231008000189'}, {'title': '[포토] 이예원, 제23회 하이트진로 챔피언십 메이저 여왕등극', 'link': 'https://www.etnews.com/20231008000188'}, {'title': '[포토] 이예원, 제23회 하이트진로 챔피언십 우승키스', 'link': 'https://www.etnews.com/20231008000187'}, {'title': '[포토] 이예원, 제23회 하이트진로 챔피언십 우승 너무 기뻐요', 'link': 'https://www.etnews.com/20231008000186'}, {'title': '[포토] 이예원, 제23회 하이트진로 챔피언십 우승 세리머니', 'link': 'https://www.etnews.com/20231008000185'}, {'title': '[포토] 이예원, 제23회 하이트진로 챔피언십 우승 키스', 'link': 'https://www.etnews.com/20231008000184'}, {'title': '[포토] 이예원, 제23회 하이트진로 챔피언십 우승', 'link': 'https://www.etnews.com/20231008000183'}, {'title': '[포토] 이예원, 우승 트로피에 시즌 3번째 키스', 'link': 'https://www.etnews.com/20231008000182'}, {'title': '[포토] 우승 트로피를 들고 기뻐하는 이예원', 'link': 'https://www.etnews.com/20231008000181'}, {'title': '글쓰기 돕는 네이버 생성형AI…내 말투 그대로 작성해 준다', 'link': 'https://www.etnews.com/20231005000273'}, {'title': \"삼성전자, 11일 '갤럭시 스마트태그2' 출시…위치 확인 기능 강화\", 'link': 'https://www.etnews.com/20231005000036'}, {'title': '퇴근하고 두 시간 더 일하면 월 사백만 원 더 벌 수 있다? [지브라도의 #트렌드로그]', 'link': 'https://www.etnews.com/20231006000208'}, {'title': 'LG U+, 초개인화 5G 요금제 16종 출시', 'link': 'https://www.etnews.com/20231005000209'}, {'title': \"BMW, 내년 韓에 전기차 충전기 '1000기' 구축\", 'link': 'https://www.etnews.com/20231005000308'}, {'title': '이통 3사, 아이폰15 시리즈 사전예약에 맞춰 혜택 제공', 'link': 'https://www.etnews.com/20231005000265'}, {'title': \"[미리 보는 테크서밋]〈1〉'OLEDoS·마이크로LED·애플' 차세대 핵심 디스플레이 기술을 듣는다\", 'link': 'https://www.etnews.com/20231005000315'}, {'title': '픽셀8, 아이폰15 등 스마트폰에 AI 채택 붐', 'link': 'https://www.etnews.com/20231006000174'}, {'title': '이통 3사, 아이폰15 공시지원금 최대 45만원', 'link': 'https://www.etnews.com/20231006000353'}, {'title': '[디지털 라이프] 세라젬 웰카페, 안마의자서 왕이 즐기던 약차 한 잔 “休~”', 'link': 'https://www.etnews.com/20231005000082'}, {'title': '[속보] 이균용 대법원장 후보자 임명동의안, 국회서 부결', 'link': 'https://www.etnews.com/20231006000175'}, {'title': \"'사상 초유' R&D 예산삭감 사태 속 과방위 국감 과학기술 분야 이슈는\", 'link': 'https://www.etnews.com/20231005000009'}, {'title': '농협 뱅킹앱서 車충전소 찾기…비금융·충전사업 확대 노린다', 'link': 'https://www.etnews.com/20231005000305'}, {'title': \"현대차그룹, 부산 새긴 '아트카' 파리 달린다\", 'link': 'https://www.etnews.com/20231006000220'}, {'title': '[ET라씨로]SV인베스트먼트 장중 상한가…왜?', 'link': 'https://www.etnews.com/20231005000261'}, {'title': \"'나는 솔로', 美친 화제성…시청률 7.9%→라방 25만명 동시 접속\", 'link': 'https://www.etnews.com/20231005000128'}, {'title': '[ET라씨로]에이스테크, 권리매도 첫날 하락세', 'link': 'https://www.etnews.com/20231006000046'}, {'title': '[ET라씨로]두산로보틱스, 상장 첫날 급등세', 'link': 'https://www.etnews.com/20231005000095'}, {'title': \"유인촌 '블랙리스트'· 김행 '주식파킹'…여야, 장관 자격 공방\", 'link': 'https://www.etnews.com/20231005000250'}]\n"
     ]
    }
   ],
   "source": [
    "# [단계 1] 모든 RSS파일(xml 형식)을 돌아다니며 기사의 제목/link 추출\n",
    "# urls는 우리가 검색할 RSS의 목록을 list로 만든 것\n",
    "urls = [\"http://rss.etnews.com/Section901.xml\",\n",
    "        \"http://rss.etnews.com/Section902.xml\",\n",
    "        \"http://rss.etnews.com/Section903.xml\",\n",
    "        \"http://rss.etnews.com/Section904.xml\"]\n",
    "\n",
    "# 아래 함수는 RSS 목록의 list안에 존재하는 모든 기사의 title, link를 list로 구성\n",
    "def crawl_rss(urls):\n",
    "    array_rss = []                                                      # 함수 시작하는 시점에 빈 list 생성(이 list에 모든 기사 채움)\n",
    "    titles_rss = set()                                                  # [중복기사제거] 기사 제목들의 집합을 구성(중복 X)\n",
    "\n",
    "    for url in urls:                                                    # 4개의 RSS파일 하나씩 방문(4번)\n",
    "        print(\"Crawl RSS\", url)                                         # 현재 위치 출력\n",
    "        parse_rss = feedparser.parse(url)                               # 현재 url 파싱 후 결과를 parse_rss에 저장\n",
    "        \n",
    "        for p in parse_rss.entries:                                     # parse_rss에 있는 모든 entries/기사 검색\n",
    "            if p.title not in titles_rss:                               # [중복기사제거] 만약에 titles_rss에 동일한 제목이 없다면 추가\n",
    "                array_rss.append({'title': p.title, 'link': p.link})    # 기사에서 제목/link 추출 후 리스트에 추가\n",
    "                titles_rss.add(p.title)                                 # [중복기사제거] 집합에 현재 기사제목이 없을 때만 추가\n",
    "            else:\n",
    "                print(\"Duplicated Article:\", p.title)                   # [중복기사제거] 중복되는 기사 제목 출력\n",
    "    \n",
    "    return array_rss\n",
    "\n",
    "list_articles = crawl_rss(urls)\n",
    "print(list_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting html5lib\n",
      "  Downloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
      "     -------------------------------------- 112.2/112.2 kB 6.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: webencodings in c:\\users\\gkwjd\\anaconda3\\lib\\site-packages (from html5lib) (0.5.1)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\gkwjd\\anaconda3\\lib\\site-packages (from html5lib) (1.16.0)\n",
      "Installing collected packages: html5lib\n",
      "Successfully installed html5lib-1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\gkwjd\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\gkwjd\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\gkwjd\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\gkwjd\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\gkwjd\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\gkwjd\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\gkwjd\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Crawl Article] https://www.etnews.com/20231006000352\n",
      "[Crawl Article] https://www.etnews.com/20231006000421\n",
      "[Crawl Article] https://www.etnews.com/20231008000006\n",
      "[Crawl Article] https://www.etnews.com/20231006000311\n",
      "[Crawl Article] https://www.etnews.com/20231006000154\n",
      "[Crawl Article] https://www.etnews.com/20231006000205\n",
      "[Crawl Article] https://www.etnews.com/20231006000081\n",
      "[Crawl Article] https://www.etnews.com/20231008000209\n",
      "[Crawl Article] https://www.etnews.com/20231008000208\n",
      "[Crawl Article] https://www.etnews.com/20231006000217\n",
      "[Crawl Article] https://www.etnews.com/20231008000207\n",
      "[Crawl Article] https://www.etnews.com/20231008000206\n",
      "[Crawl Article] https://www.etnews.com/20231008000205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gkwjd\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Crawl Article] https://www.etnews.com/20231008000204\n",
      "[Crawl Article] https://www.etnews.com/20231008000203\n",
      "[Crawl Article] https://www.etnews.com/20231008000202\n",
      "[Crawl Article] https://www.etnews.com/20231008000201\n",
      "[Crawl Article] https://www.etnews.com/20231008000200\n",
      "[Crawl Article] https://www.etnews.com/20231008000199\n",
      "[Crawl Article] https://www.etnews.com/20231008000198\n",
      "[Crawl Article] https://www.etnews.com/20231008000197\n",
      "[Crawl Article] https://www.etnews.com/20231008000196\n",
      "[Crawl Article] https://www.etnews.com/20231008000195\n",
      "[Crawl Article] https://www.etnews.com/20231008000194\n",
      "[Crawl Article] https://www.etnews.com/20231008000193\n",
      "[Crawl Article] https://www.etnews.com/20231008000192\n",
      "[Crawl Article] https://www.etnews.com/20231008000191\n",
      "[Crawl Article] https://www.etnews.com/20231008000189\n",
      "[Crawl Article] https://www.etnews.com/20231008000188\n",
      "[Crawl Article] https://www.etnews.com/20231008000187\n",
      "[Crawl Article] https://www.etnews.com/20231008000186\n",
      "[Crawl Article] https://www.etnews.com/20231008000185\n",
      "[Crawl Article] https://www.etnews.com/20231008000184\n",
      "[Crawl Article] https://www.etnews.com/20231008000183\n",
      "[Crawl Article] https://www.etnews.com/20231008000182\n",
      "[Crawl Article] https://www.etnews.com/20231008000181\n",
      "[Crawl Article] https://www.etnews.com/20231005000273\n",
      "[Crawl Article] https://www.etnews.com/20231005000036\n",
      "[Crawl Article] https://www.etnews.com/20231006000208\n",
      "[Crawl Article] https://www.etnews.com/20231005000209\n",
      "[Crawl Article] https://www.etnews.com/20231005000308\n",
      "[Crawl Article] https://www.etnews.com/20231005000265\n",
      "[Crawl Article] https://www.etnews.com/20231005000315\n",
      "[Crawl Article] https://www.etnews.com/20231006000174\n",
      "[Crawl Article] https://www.etnews.com/20231006000353\n",
      "[Crawl Article] https://www.etnews.com/20231005000082\n",
      "[Crawl Article] https://www.etnews.com/20231006000175\n",
      "[Crawl Article] https://www.etnews.com/20231005000009\n",
      "[Crawl Article] https://www.etnews.com/20231005000305\n",
      "[Crawl Article] https://www.etnews.com/20231006000220\n",
      "[Crawl Article] https://www.etnews.com/20231005000261\n",
      "[Crawl Article] https://www.etnews.com/20231005000128\n",
      "[Crawl Article] https://www.etnews.com/20231006000046\n",
      "[Crawl Article] https://www.etnews.com/20231005000095\n",
      "[Crawl Article] https://www.etnews.com/20231005000250\n",
      "{'title': \"2000년 전 로마 유적지에서 발견된 '핑크빛 아이섀도'\", 'link': 'https://www.etnews.com/20231006000352', 'text': '튀르키예(터키)에서 2000년 전 로마인들이 사용한 색조 화장품이 아직까지 선명한 색깔을 유지한 모습으로 발굴됐다.\\n\\n\\n\\n최근 튀르키예 국영 통신사 아난돌루에 따르면, 서부 고대 도시 아이자노이 유적지에서 2000여 년 전 로마인들이 사용했을 것으로 추정되는 보석과 화장품이 발굴돼 대중에 공개됐다.\\n\\n\\n\\n이 지역은 과거 그리스 문화권에 속했으며, 고대에는 이오니아로 불린 곳이다. 프리기아인이 거주하다가 기원전 1세기부터 로마 도시로 변모했다.\\n\\n\\n\\n화장품이 발굴된 곳은 지난 2012년 유네스코 세계문화유산 잠정 목록에 오른 제우스 신전 동쪽 아고라(시장)다. 발굴팀은 상점들의 매장 내부는 물론 주변까지 꼼꼼하게 발굴 작업을 진행했다.\\n\\n발굴 책임자인 고칸 코스쿤 두물루피나 대학 고고학자는 “완전히 발굴한 곳은 향수, 쥬얼리, 메이크업 재료 등 화장품을 판매하는 매장으로 파악된다”며 “수많은 향수병은 물론 보석들과 여성 머리핀, 목걸이를 장식한 구슬들이 발견됐다”고 말했다.\\n\\n\\n\\n특히 놀라운 발견은 색조 화장품이었다. 눈이나 볼에 사용했을 것으로 보이는 분홍색 화장품이 거의 완전한 형태로 발견된 경우가 있었다.\\n\\n\\n\\n코스쿤 책임자는 “블러셔나 아이섀도우 용도의 안료를 패류 껍데기 안에 넣어서 사용하는 경우가 있었다. 물론 발굴지에서도 수많은 조개 껍데기가 발견됐다”면서 “분홍색 외에도 빨간색 등 10가지 색상이 발견됐다”고 전했다.\\n\\n\\n\\n전자신문인터넷 서희원 기자 shw@etnews.com'}\n"
     ]
    }
   ],
   "source": [
    "# [단계 2] list에 존재하는 모든 링크를 돌아다니며 본문 text를 추출\n",
    "\n",
    "# 아래 함수는 하나의 url을 입력 받아서, 링크를 타고 들어가, 그 안에 title과 text를 추출한다.\n",
    "# default로 한글을 지정\n",
    "def crawl_article(url, language='ko'):\n",
    "    print(\"[Crawl Article]\", url)                               # 현재 title과 text를 추출한 url을 프린트\n",
    "    a = Article(url, language=language)                         # Article을 사용하여 그 url을 입력하고, 언어옵션 지정, a에 저장\n",
    "    a.download()                                                # a에 해당하는 url기사 다운로드\n",
    "    a.parse()                                                   # a에 해당하는 url기사 분석\n",
    "\n",
    "    return a.title, preprocessing(a.text)\n",
    "\n",
    "def preprocessing(text):\n",
    "    # html 태그 제거\n",
    "    text_article = BeautifulSoup(text, 'html5lib').get_text()\n",
    "\n",
    "    return text_article\n",
    "\n",
    "for article in list_articles:                                   # list에 있는 모든 기사를 하나씩 방문\n",
    "    _, text = crawl_article(article['link'])                    # 그 기사의 link를 crawl_article 함수에 넣어 본문 추출\n",
    "    article['text'] = text                                      # 추출된 본문을 list_articles에 'text'라는 속성 새로 만들어 저장\n",
    "\n",
    "print(list_articles[0])                                         # 첫 번째 기사를 출력(title, link, text가 모두 나오는 것 확인)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': \"2000년 전 로마 유적지에서 발견된 '핑크빛 아이섀도'\", 'link': 'https://www.etnews.com/20231006000352', 'text': '튀르키예(터키)에서 2000년 전 로마인들이 사용한 색조 화장품이 아직까지 선명한 색깔을 유지한 모습으로 발굴됐다.\\n\\n\\n\\n최근 튀르키예 국영 통신사 아난돌루에 따르면, 서부 고대 도시 아이자노이 유적지에서 2000여 년 전 로마인들이 사용했을 것으로 추정되는 보석과 화장품이 발굴돼 대중에 공개됐다.\\n\\n\\n\\n이 지역은 과거 그리스 문화권에 속했으며, 고대에는 이오니아로 불린 곳이다. 프리기아인이 거주하다가 기원전 1세기부터 로마 도시로 변모했다.\\n\\n\\n\\n화장품이 발굴된 곳은 지난 2012년 유네스코 세계문화유산 잠정 목록에 오른 제우스 신전 동쪽 아고라(시장)다. 발굴팀은 상점들의 매장 내부는 물론 주변까지 꼼꼼하게 발굴 작업을 진행했다.\\n\\n발굴 책임자인 고칸 코스쿤 두물루피나 대학 고고학자는 “완전히 발굴한 곳은 향수, 쥬얼리, 메이크업 재료 등 화장품을 판매하는 매장으로 파악된다”며 “수많은 향수병은 물론 보석들과 여성 머리핀, 목걸이를 장식한 구슬들이 발견됐다”고 말했다.\\n\\n\\n\\n특히 놀라운 발견은 색조 화장품이었다. 눈이나 볼에 사용했을 것으로 보이는 분홍색 화장품이 거의 완전한 형태로 발견된 경우가 있었다.\\n\\n\\n\\n코스쿤 책임자는 “블러셔나 아이섀도우 용도의 안료를 패류 껍데기 안에 넣어서 사용하는 경우가 있었다. 물론 발굴지에서도 수많은 조개 껍데기가 발견됐다”면서 “분홍색 외에도 빨간색 등 10가지 색상이 발견됐다”고 전했다.\\n\\n\\n\\n전자신문인터넷 서희원 기자 shw@etnews.com', 'keywords': [{'keyword': '발굴', 'count': 7}, {'keyword': '화장품', 'count': 6}, {'keyword': '발견', 'count': 5}, {'keyword': '사용', 'count': 4}, {'keyword': '전', 'count': 3}, {'keyword': '곳', 'count': 3}, {'keyword': '튀르키예', 'count': 2}, {'keyword': '로마인', 'count': 2}, {'keyword': '색조', 'count': 2}, {'keyword': '고대', 'count': 2}]}\n"
     ]
    }
   ],
   "source": [
    "# [단계 3] 모든 본문 text에서 명사(키워드, 빈도수) 추출\n",
    "def get_keywords(text, nKeywords=10):           # 키워드 추출 함수, default : 10개\n",
    "    spliter = Okt()                             # konlpy에 의해 문장을 형태소별로 쪼개는 기능을 위해 spliter 생성\n",
    "    nouns = spliter.nouns(text)                 # spliter에 의해서 nouns 함수를 불러 text를 넣으면 그 text의 명사만 추출\n",
    "    count = Counter(nouns)                      # 추출된 명사들의 출현빈도 추출\n",
    "    list_keywords = []                          # 비어있는 키워드 리스트 생성\n",
    "\n",
    "    for n, c in count.most_common(nKeywords):   # 가장 출현 빈도 높은 명사부터 순차적으로 10번 출력\n",
    "        item = {'keyword': n, 'count': c}       # 리스트에 저장하기 위해 dictionary 형태로 {'keyword', 'count'}\n",
    "        list_keywords.append(item)              # list_keywords에 추가\n",
    "\n",
    "    return list_keywords\n",
    "\n",
    "for article in list_articles:                   # 모든 기사 돌아다니며 text에서 명사 추출, 키워드/빈도 추출\n",
    "    keywords = get_keywords(article['text'])    # get_keywords 함수로 키워드/빈도 추출\n",
    "    article['keywords'] = keywords              # 추출된 키워드/빈도를 list_articles의 'keyword'로 저장\n",
    "\n",
    "print(list_articles[0])                         # 첫 번째 기사 출력(title, link, text, keywords/빈도 쌍 최대 10개)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TF] 6 [Title] '사상 초유' R&D 예산삭감 사태 속 과방위 국감 과학기술 분야 이슈는 [URL] https://www.etnews.com/20231005000009\n"
     ]
    }
   ],
   "source": [
    "# [단계 4] 검색어를 입력 받아서 그 검색어를 가지고 있는 기사를 출력\n",
    "query = input()\n",
    "\n",
    "def search_articles(query, list_keywords):\n",
    "    nWords = 0\n",
    "    for kw in list_keywords:\n",
    "        if eq(query, kw['keyword']):\n",
    "            nWords = kw['count']\n",
    "\n",
    "    return nWords\n",
    "\n",
    "for article in list_articles:\n",
    "    nQuery = search_articles(query, article['keywords'])\n",
    "\n",
    "    if nQuery != 0:\n",
    "        print('[TF]', nQuery, '[Title]', article['title'], '[URL]', article['link'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
